{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models layer simple machine learning models and trains the composition jointly, so that earlier layers can learn features which are useful to later layers. \n",
    "\n",
    "The simplest neural network is the **multilayer perceptron** (MLP). Consider a set of training data of the form $\\{(\\mathbf{x}_1,\\mathbf{y}_1), \\ldots, (\\mathbf{x}_n,\\mathbf{y}_n)\\}$ where each input $\\mathbf{x}_i$ is in $\\mathbb{R}^p$ and each output $\\mathbf{y}_i$ is in $\\mathbb{R}^q$. The idea of the MLP is to model the input-output relationship as a function which alternately applies linear transformations (of the form $A(\\mathbf{x}) = W\\mathbf{x} + \\mathbf{b}$) and component-wise applications of the nonlinear _ReLU_ function $\\operatorname{ReLU}(x) = \\max(0,x)$. We call this nonlinear function the model's **activation function**. The **depth** of the neural network is the number of layers. \n",
    "\n",
    "Here's a simple example, illustrated three ways: (1) in Python, (2) using a vector-based **computational graph**, and (3) using a component-based computational graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1, -1]), 5.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "x  = np.array([1,-2,3])\n",
    "W1 = np.array([[4,2,-1],[5,-2,-2]])\n",
    "b1 = np.array([0,1])\n",
    "W2 = np.array([[0,1],[-2,2],[3,4],[-2,0]])\n",
    "b2 = np.array([-1,-7,-14,-5])\n",
    "W3 = np.array([[-2,3,-6,4],[-1,0,-1,0]])\n",
    "b3 = np.array([14, 4])\n",
    "output = W3 @ ReLU(W2 @ ReLU(W1 @ x + b1) + b2) + b3\n",
    "output, np.linalg.norm(output - np.array([3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn.png\">\n",
    "<img src=\"nodes-nn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Exercise**. Why is it necessary to include a nonlinear function like ReLU for a MLP's expressive power to increase as the depth is increased?_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The next task is to identify parameters such that the model accurately reflects the input-output relationship for a given set of data. This is done using called **backpropagation**. The idea is to measure the error (or **loss**) of an output value $\\widehat{\\mathbf{y}}$ given the desired output $\\mathbf{y}$ and take the derivative of this error value with respect to each of the weights in the model. These derivatives can be averaged over a set of training samples, and that information can be used to nudge each weight in a direction that decreases average error. \n",
    "\n",
    "_**Exercise**. Compute a rough estimate of the derivative of the loss in the example above with respect to the top-left entry of $W_1$ by changing that value by a small amount and determining the resulting change in loss. Based on that information, should the value in that position be adjusted up or down (to decrease the loss)?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems, it's common practice to let the number of dimensions in the last layer to be equal to the number of classes (so that each node corresponds to a particular class) and apply the **softmax** function* at that layer so that the output values are nonnegative and sum to 1. \n",
    "\n",
    "*The softmax function exponentiates each entry of a vector and then divides each entry by the sum of all of the entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26762315, 0.72747516, 0.00490169])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.exp(np.array([1,2,-3])) # exponentiate\n",
    "a /= np.sum(a) # normalize\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret the values as confidence values, or *probabilities*, for each class. For example, a neural network which returns the vector `a` (above) for a particular input would be expressing a strong belief that that input is not in the third class, and is more likely to be in the second class than the first. \n",
    "\n",
    "Usually when we use softmax in the last layer of a classification problem, we also use the *cross-entropy loss* function, which returns the natural logarithm of the reciprocal of the value in the position of the correct class. For example, if the correct class were 2 and the neural network output `a`, the loss would be $-\\log(0.727) = 0.319$, while a correct class of 3 would result in a much larger penalty of $-\\log(0.0049) = 5.32$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Keras is a Python module (built into the `tensorflow` module) that supports convenient layer-by-layer model building and automatic differentiation for training. Let's see a neural network in action. \n",
    "\n",
    "This example draws from the notebook at https://www.tensorflow.org/tutorials/keras/basic_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACndJREFUeJzt3U1LVXscxfFtD2pmpqmlUXQLCho0qSZBNe81BI2jUb2AXkCTBk0Kit6AUC8gKIrogQgqCCwEEcsy8yH1aNrTncgdtdfqusuH1vczvIv/Ofsc77ob7u/8/7vux48fBYC/35rlvgAAS4OyAyEoOxCCsgMhKDsQYt0Sv99f+b/+a7WazM+fPy/zBw8eyPzUqVMyP3PmjMxXq56eHplfu3ZN5idOnCjNzp49u6hrWiXqfvYPubMDISg7EIKyAyEoOxCCsgMhKDsQgrIDIeqWeNfbqp2znz59ujS7e/euXPv9+3eZb9u2TeYvX76UeWdnZ2m2c+dOuXbv3r0y37x5s8zHxsZkrn5DMD8/L9dOTk7KvLu7W+bq9w87duyQa69evSrzPXv2yHyZMWcHklF2IARlB0JQdiAEZQdCUHYgBGUHQjBnX3D79m2ZX7hwoTRrb2+Xa9282M3hP3/+LPORkZHSzO217+rqkvnhw4dl/uTJE5mra29tbZVr3e8PPnz4IPO2trbSbGJiQq5taWmR+c2bN2W+zJizA8koOxCCsgMhKDsQgrIDISg7EGKpj5JesW7duiXzf/75pzSbm5uTa9evXy/zL1++yLyjo0Pm69aV/xndaPXbt28yd9trN2zYIPPm5ubSbNOmTXLt27dvZd7U1CRz9dndFlc3Lr1//77Mjx49KvPlwJ0dCEHZgRCUHQhB2YEQlB0IQdmBEJQdCMGcfcHQ0JDM1ZbHqnN2N+t2r19fX1+aqTl3UfjjnJ21a9fKXM2rZ2Zm5Fo3R3efbc2a8nuZ+87r6n66S/Q/zNkBrFiUHQhB2YEQlB0IQdmBEJQdCEHZgRAxc3Z3XLPbv6weXewea+yOgnbcfnc1M56enpZrv379KnM1wy8Kf23qe3fv7f5m7r0bGxtlrrg5++vXrxf92suFOzsQgrIDISg7EIKyAyEoOxCCsgMhKDsQImbO3t/fL3M3052dnS3N3ON91aODi8LPi6empmSuzo13+9XdufLuNwJuvdrL7+bs7rXdLFztZ3d75R13pv1KxJ0dCEHZgRCUHQhB2YEQlB0IQdmBEDGjt3fv3sm8oaFB5mqM40ZEu3btkrk71tg92li9v9vi6o65Vp/7V9arsaJ73LM7ptptv+3u7i7NarWaXOu+t/b2dpmPjIzIvLOzU+Z/And2IARlB0JQdiAEZQdCUHYgBGUHQlB2IETMnH10dFTmaiZbFEXx6dOn0uzevXty7cmTJ2W+fft2mbvfCKhHOrtZtptVO2p7rXt9t8XVvfbWrVtl/ujRo9LM/X5g//79MndHj/f29sqcOTuAP4ayAyEoOxCCsgMhKDsQgrIDISg7ECJmzu72F7vjmu/cubPo13769KnMjx8/LvMXL17IvLW1tTRzc3R3hLbbr+6OqlazdHdMtdtz7s4JUMdFP378WK5117Zjxw6ZP3/+XObHjh2T+Z/AnR0IQdmBEJQdCEHZgRCUHQhB2YEQlB0IUefOPP/NlvTN/o+BgQGZnzt3rjS7dOmSXHv9+nWZu8f/ut8AqEdGuzm44+bw7t8fdfb7zMyMXDs8PCxztye9p6enNLt48aJc6/4mV65ckbl7DsEf9tNnWXNnB0JQdiAEZQdCUHYgBGUHQlB2IARlB0IwZ18CN27ckPnly5dl7vZOqz3r6vnov8Kd7e7m8Irab14URdHf3y9z91z727dv/+9r+kswZweSUXYgBGUHQlB2IARlB0JQdiBEzFHSbsToRkgqd8ctHzhwQObNzc0yr6v76STlP+raqj4W2W0jddT7u8/lRnODg4OLuqZf4cZ6jtrau1y4swMhKDsQgrIDISg7EIKyAyEoOxCCsgMhYubsbqbr5qJV5s1uju64xy6rxwu7ObqbJ1eZ8ReF/t7cY5E3btwoc/e9VOH+3u57WYm4swMhKDsQgrIDISg7EIKyAyEoOxCCsgMhYubsVal5tJtlu+Oc3Xq3X356ero027Bhg1zrZt3uvd2cXX222dlZudbN0fft2yfzKtz5B8zZAaxYlB0IQdmBEJQdCEHZgRCUHQhB2YEQzNmXwNu3b2XuZtluFq7UarVK7+24fd/qNwbu9wVV9soXRVG8efOmNHOPwV7iR5kvCe7sQAjKDoSg7EAIyg6EoOxACMoOhKDsQAjm7L+oyv7lhw8fytzNuufn52Wu5tENDQ1yrdtT7ta78/bV67tz4d2z5d21f/jwoTRzc3Y341+Jz193uLMDISg7EIKyAyEoOxCCsgMhKDsQgtHbL6ryyOa+vj6ZVxlfFYUezbnRWdVjrquMoNzW3aamJpm7a3/16lVpdvDgQbl2NR4V7XBnB0JQdiAEZQdCUHYgBGUHQlB2IARlB0IwZ19Q5dhitxVzZGRE5m4W7ma+VY49dttn3bW5Oby6Njejd38TtzVYzdmdKr+rWKn+vk8E4KcoOxCCsgMhKDsQgrIDISg7EIKyAyGYsy+oMquenJyUeXt7u8zVkcdFURQtLS0yn5qaKs3cLPrbt28yd9xvDNT36t7b/b7Avbc7R0Bxc3b378tK3A/PnR0IQdmBEJQdCEHZgRCUHQhB2YEQlB0IwZx9QZU5++DgoMzdHN7NZOfm5mSu9qS713Znr7uz3RsbG2Wu3t+dh79p0yaZu7309fX1pZn73O73Cavxkc7c2YEQlB0IQdmBEJQdCEHZgRCUHQjB6O036O3tlbkbvW3ZskXm4+PjMq8yYnLbRKuO3tS1TUxMyLVu/OXeW137p0+f5NqOjg6ZVxnVLhfu7EAIyg6EoOxACMoOhKDsQAjKDoSg7EAI5uy/wdjYmMzdFlW3VdPNhNVR1VWPa3ZbOd2cvrm5uTRzc3a3xdVdm/rs79+/l2vdnH014s4OhKDsQAjKDoSg7EAIyg6EoOxACMoOhGDOvqDK/uT+/n6Zu33ZzvT0tMz37NlTmrkZv+Nm/G1tbTJX+9nd53JHTTc0NMhczeHVY65/BfvZAaxYlB0IQdmBEJQdCEHZgRCUHQhB2YEQzNl/A/d4Xne+edV5sprjq8c5F0VR1Go1mbu9+rt375a5e3/F7cV337s6M9/t43fcXvqViDs7EIKyAyEoOxCCsgMhKDsQgrIDISg7EII5+2+g9mwXhZ8Hu1n01q1bZb5mTfl/s92M3r23u3b3bPmZmZnSbOPGjXKt2zNeZVbufvvgqO98pVp9VwxgUSg7EIKyAyEoOxCCsgMhKDsQgtHbb/D69WuZu0cTu6Omx8fHF5270dro6KjMJycnZd7X1yfz4eHh0uzZs2dy7ZEjR2TujqJWozs3Lv0bcWcHQlB2IARlB0JQdiAEZQdCUHYgBGUHQjBnX1Bly+Lhw4dl/vHjR5m7Laxum2pnZ2dp5raoDg0NVcoPHTokc/XI6IGBAbnWbWFtamqSuZrjd3V1ybUOW1wBrFiUHQhB2YEQlB0IQdmBEJQdCEHZgRB17rheAH8H7uxACMoOhKDsQAjKDoSg7EAIyg6EoOxACMoOhKDsQAjKDoSg7EAIyg6EoOxACMoOhKDsQAjKDoSg7EAIyg6EoOxACMoOhKDsQAjKDoSg7ECIfwFYegM3fSYw5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[10,:,:], cmap=plt.cm.binary)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
